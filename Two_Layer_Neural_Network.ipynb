{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HrishitaC_Assignment_9_Two_Layer_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXmQGNBWwzS1",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "- In this assignment, you will build a two layer neural network for classification from scratch using only numpy.\n",
        "- Please refer to videos on Backpropagation and one reference material shared in additional resources for the understanding required to solve this assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCZeHgK-xAy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Some functions required for testing \"\"\"\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(D, H, C):\n",
        "  il = keras.layers.Input(shape=(D,))\n",
        "  hl = keras.layers.Dense(H, activation = 'relu')(il)\n",
        "  ol = keras.layers.Dense(C, activation = 'softmax')(hl)\n",
        "  model = keras.models.Model(inputs = [il], outputs = [ol])\n",
        "\n",
        "  rng = np.random.RandomState(2020)\n",
        "  model.layers[1].set_weights([rng.rand(D * H).reshape(D, H), rng.rand(H, )])\n",
        "  model.layers[2].set_weights([rng.rand(H * C).reshape(H, C), rng.rand(C, )])\n",
        "  return model\n",
        "\n",
        "def create_inputs(N, D):\n",
        "  rng = np.random.RandomState(2020)\n",
        "  return rng.rand(N * D).reshape(N, D)\n",
        "\n",
        "def set_weights_from_model(tln, test_net):\n",
        "  tln.params['W1'] = test_net.layers[1].get_weights()[0]\n",
        "  tln.params['b1'] = test_net.layers[1].get_weights()[1]\n",
        "  tln.params['W2'] = test_net.layers[2].get_weights()[0]\n",
        "  tln.params['b2'] = test_net.layers[2].get_weights()[1]\n",
        "  return tln\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9OoL62wOux9t",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network. The net has an input dimension of\n",
        "    D, a hidden layer dimension of H, and performs classification over C classes.\n",
        "    We train the network with a softmax loss function and L2 regularization on the\n",
        "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
        "    connected layer.\n",
        "\n",
        "    In other words, the network has the following architecture:\n",
        "\n",
        "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "    The outputs of the second fully-connected layer are the scores for each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
        "        \"\"\"\n",
        "        Initialize the model. \n",
        "        Weights are initialized to small random values and\n",
        "        biases are initialized to zero. \n",
        "        Weights and biases are stored in the\n",
        "        variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "\n",
        "        Inputs:\n",
        "        - input_size: The dimension N of the input data.\n",
        "        - hidden_size: The number of neurons H in the hidden layer.\n",
        "        - output_size: The number of classes C.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ### Write your code here\n",
        "        W1 = ((2*np.random.random((input_size, hidden_size)))  - 1)*0.01/2\n",
        "        b1 = np.zeros(shape = (hidden_size, ))\n",
        "        W2 = ((2*np.random.random((hidden_size, output_size))) - 1)*0.01/2\n",
        "        b2 = np.zeros(shape = (output_size, ))\n",
        "        self.params = {\n",
        "            \"W1\": W1,\n",
        "            \"b1\": b1,\n",
        "            \"W2\": W2,\n",
        "            \"b2\": b2\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ScjBCeTwzS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d80fa825-89c5-4f4f-cda2-86ee2cf72c40"
      },
      "source": [
        "\"\"\" Test Cases for Initialization\"\"\"\n",
        "tln = TwoLayerNet(2, 3, 2)\n",
        "assert tln.params['W1'].shape == (2, 3)\n",
        "assert tln.params['b1'].shape == (3, )\n",
        "assert tln.params['W2'].shape == (3, 2)\n",
        "assert tln.params['b2'].shape == (2, )\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kh8LRCT9voz1",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "\n",
        "    def forward(self, X):\n",
        "      \"\"\"\n",
        "      Compute the output of a full forward pass of the network.\n",
        "      \n",
        "      First apply weights W1 and biases b1 on inputs and then apply relu non-linearity.\n",
        "      Then apply weights W2 and biases b2 on hidden layer values and then apply softmax non-linearity to get the output\n",
        "      \n",
        "      Inputs:\n",
        "      - X : Input data of shape (N, D). Each X[i] is a training sample\n",
        "      \n",
        "      Outputs:\n",
        "      - y_out : numpy array with Outputs of shape (N, C)\n",
        "      \n",
        "      \"\"\"\n",
        "      ### Write your code here\n",
        "      W1 = self.params[\"W1\"]\n",
        "      W2 = self.params[\"W2\"]\n",
        "      b1 = self.params[\"b1\"]\n",
        "      b2 = self.params[\"b2\"]\n",
        "\n",
        "      v1 = np.dot(X, W1) + b1\n",
        "      h1 = np.maximum(v1, 0)\n",
        "      v2 = np.dot(h1, W2) + b2\n",
        "      h2 = np.zeros((v2.shape[0],v2.shape[1]))\n",
        "      for i in range(v2.shape[0]):\n",
        "        h2[i] = (np.exp(v2[i])/sum(np.exp(v2[i])))\n",
        "      y_out = h2\n",
        "      return y_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJeXhkRZwzTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "878a314d-5987-4a04-d128-aa274b25ca5a"
      },
      "source": [
        "\"\"\"Test Cases for Forward pass\"\"\"\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "test_net = create_model(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, test_net)\n",
        "X = create_inputs(4, 2)\n",
        "y_forward = tln.forward(X)\n",
        "assert y_forward.shape == (4, 2)\n",
        "assert np.all(np.isclose(y_forward, test_net.predict(X), atol = 0.0001))\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbOelhQCwzTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this two-layer network to predict labels for\n",
        "        data points. For each data point we predict scores for each of the C\n",
        "        classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
        "          classify.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
        "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
        "          to have class c, where 0 <= c < C.\n",
        "        \"\"\"\n",
        "        y_pred = None\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Implement this function; it should be VERY simple!                #\n",
        "        ###########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        ### Write your code here\n",
        "        y_pred = []\n",
        "        y_out = self.forward(X)\n",
        "        for i in range(y_out.shape[0]):\n",
        "          y_pred.append(np.argmax(y_out[i]))\n",
        "        y_pred = np.asarray(y_pred, dtype=np.int_)\n",
        "        return y_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAJuwv5bwzTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dae69181-235b-4c4f-e2eb-e454a43d2b15"
      },
      "source": [
        "\"\"\" Test Cases for predict\"\"\"\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "test_net = create_model(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, test_net)\n",
        "X = create_inputs(4, 2)\n",
        "y_pred = tln.predict(X)\n",
        "test_pred = np.argmax(test_net.predict(X), axis = 1)\n",
        "assert np.all(np.isclose(y_pred, test_pred, atol = 0.01))\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabhV02TwzTO",
        "colab_type": "text"
      },
      "source": [
        "#### Loss\n",
        "Note: <br>\n",
        "$L = -\\sum{t_i \\log{p_i}}$ <br>\n",
        "where $p_i$ is probability score predicted by model. <br>\n",
        "$t_i = 0$ for the true class $i$ and $t_i = 1$ for all other classes for a particular sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mO3-DSCwzTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):    \n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute the loss and gradients for a two layer fully connected neural\n",
        "        network.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "          an integer in the range 0 <= y[i] < C.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
        "        the score for class c on input X[i].\n",
        "\n",
        "        If y is not None, instead return a tuple of:\n",
        "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "          samples. (This is the mean loss over N samples)\n",
        "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "          with respect to the loss function; has the same keys as self.params.\n",
        "        \"\"\"\n",
        "        # Unpack variables from the params dictionary\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        N, D = X.shape\n",
        "        H, C = W2.shape\n",
        "        \n",
        "        # Compute the forward pass\n",
        "        scores = None\n",
        "        #############################################################################\n",
        "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
        "        # Store the result in the scores variable, which should be an array of      #\n",
        "        # shape (N, C).                                                             #\n",
        "        #############################################################################\n",
        "        \n",
        "        \n",
        "        \n",
        "        ## Write your code here\n",
        "        \n",
        "        v1 = np.dot(X, W1) + b1\n",
        "        h1 = np.maximum(v1, 0)\n",
        "        v2 = np.dot(h1, W2) + b2\n",
        "        h2 = []\n",
        "        for i in range(v2.shape[0]):\n",
        "          h2.append(np.exp(v2[i])/sum(np.exp(v2[i])))\n",
        "        scores = np.asarray(h2)\n",
        "\n",
        "        y = np.asarray(y)\n",
        "        if(y.all()==None):\n",
        "          return scores\n",
        "        \n",
        "        # # Compute the loss\n",
        "        loss = None\n",
        "\n",
        "        \n",
        "        #############################################################################\n",
        "        # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
        "        # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
        "        # in the variable loss, which should be a scalar. Use the Categorical       #\n",
        "        # Cross Entropy loss.                                                       #\n",
        "        #############################################################################\n",
        "      \n",
        "        ### Write your code here\n",
        "\n",
        "        t_i = np.zeros((N,C), dtype=np.int_)\n",
        "        for i in range(N):\n",
        "          for j in range(C):\n",
        "            if (j==y[i]):\n",
        "              t_i[i,j] = 1\n",
        "\n",
        "        data_loss = np.sum(np.sum(t_i*np.log(scores)))\n",
        "\n",
        "        data_loss = -data_loss/N\n",
        "        lam = 0.001\n",
        "\n",
        "        l2 = (lam/(2*N) * np.sum(W1** 2)) + (lam/(2*N) * np.sum(W2 ** 2))\n",
        "\n",
        "        loss = data_loss #+ l2\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        grads = {}\n",
        "        \n",
        "        #############################################################################\n",
        "        # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
        "        # and biases. Store the results in the grads dictionary. For example,       #\n",
        "        # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
        "        #############################################################################\n",
        "        \n",
        "        ### Write your code here\n",
        "\n",
        "        output_error = (scores-t_i)/scores.shape[0] #dl/d(softmax) = (pi - yi)/N\n",
        "        grad_W2 = np.dot(h1.T, output_error)\n",
        "        grad_b2 = np.sum(output_error, axis=0)\n",
        "        hidden_error = np.dot((scores-t_i)/scores.shape[0], W2.T)\n",
        "        grad_W1 = np.dot(X.T, hidden_error)\n",
        "        grad_b1 = np.sum(hidden_error, axis=0)\n",
        "\n",
        "        grads = {\n",
        "            \"W1\": grad_W1,\n",
        "            \"b1\": grad_b1,\n",
        "            \"W2\": grad_W2,\n",
        "            \"b2\": grad_b2\n",
        "        }\n",
        "\n",
        "        return loss, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn1tAYrpwzTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a43d3f7e-0470-438a-ceef-2d9f439913bb"
      },
      "source": [
        "\"\"\" Tests for loss and gradient computation \"\"\"\n",
        "### First compute loss and gradients using keras\n",
        "model = create_model(2, 4, 2)\n",
        "X = create_inputs(4, 2)\n",
        "y = np.array([0, 1, 1, 0])\n",
        "y_onehot = keras.utils.to_categorical(y, 2)\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "batch_size = 4\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X, y_onehot))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_out = model(x_batch_train, training = True)\n",
        "\n",
        "      ## Compute loss value for this minibatch\n",
        "      loss_value = loss_fn(y_batch_train, y_out)\n",
        "    \n",
        "    grads_model = {}\n",
        "    grads_model['W1'], grads_model['b1'], grads_model['W2'], grads_model['b2'] = [dw.numpy() for dw in tape.gradient(loss_value, model.trainable_weights)]\n",
        "\n",
        "### Compute loss and gradients using TwoLayerNet\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, model)\n",
        "loss, grads_tln = tln.loss(X, y)\n",
        "\n",
        "#### Now match\n",
        "## Loss should be correctly computed\n",
        "assert np.isclose(loss, loss_value.numpy(), atol = 0.0001)\n",
        "\n",
        "## Gradients should be correctly computed\n",
        "assert np.all(np.isclose(grads_tln['W1'], grads_model['W1'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['b1'], grads_model['b1'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['W2'], grads_model['W2'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['b2'], grads_model['b2'], atol = 0.0001))\n",
        "\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANJB4KJni0ME",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyIYI5btwzTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "    def train(self, X, y, X_val, y_val,\n",
        "              learning_rate=1e-3, num_iters=100,\n",
        "              batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this neural network using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving training data.\n",
        "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
        "          X[i] has label c, where 0 <= c < C.\n",
        "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
        "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
        "        - learning_rate: Scalar giving learning rate for optimization.\n",
        "        - num_iters: Number of steps to take when optimizing.\n",
        "        - batch_size: Number of training examples to use per step.\n",
        "        \"\"\"\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "        # Use SGD to optimize the parameters in self.model\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        np.random.shuffle(X)\n",
        "        state = np.random.get_state()\n",
        "        np.random.set_state(state)\n",
        "        np.random.shuffle(y)\n",
        "\n",
        "        for it in range(num_iters):\n",
        "              X_batch = None\n",
        "              y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO: Create a random minibatch of training data and labels, storing  #\n",
        "            # them in X_batch and y_batch respectively.                             #\n",
        "            #########################################################################\n",
        "\n",
        "            \n",
        "            ### Write your code here\n",
        "            \n",
        "              start = (num_iters * batch_size)%X.shape[0]\n",
        "              X_batch = X[start: start+batch_size]\n",
        "              y_batch = y[start: start+batch_size]\n",
        "\n",
        "\n",
        "              # Compute loss and gradients using the current minibatch\n",
        "              loss, grads = self.loss(X=X_batch, y=y_batch)\n",
        "              loss_history.append(loss)\n",
        "              # print(loss)\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO: Use the gradients in the grads dictionary to update the         #\n",
        "            # parameters of the network (stored in the dictionary self.params)      #\n",
        "            # using stochastic gradient descent. You'll need to use the gradients   #\n",
        "            # stored in the grads dictionary defined above.                         #\n",
        "            #########################################################################\n",
        "\n",
        "            \n",
        "              ### Write your code here\n",
        "              self.params[\"W1\"] -= (learning_rate * grads[\"W1\"])\n",
        "              self.params[\"b1\"] -= (learning_rate * grads[\"b1\"])\n",
        "              self.params[\"W2\"] -= (learning_rate * grads[\"W2\"])\n",
        "              self.params[\"b2\"] -= (learning_rate * grads[\"b2\"])\n",
        "            # Every epoch, check train and val accuracy\n",
        "              if it % iterations_per_epoch == 0:\n",
        "                # Check accuracy\n",
        "\n",
        "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "                val_acc = (self.predict(X_val) == y_val).mean()\n",
        "                train_acc_history.append(train_acc)\n",
        "                val_acc_history.append(val_acc)\n",
        "\n",
        "                # Decay learning rate\n",
        "                learning_rate *= 1\n",
        "\n",
        "        return {\n",
        "          'loss_history': loss_history,\n",
        "          'train_acc_history': train_acc_history,\n",
        "          'val_acc_history': val_acc_history,\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYEfUA1nmCju",
        "colab_type": "text"
      },
      "source": [
        "### Using these networks on datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsACefMSmFX0",
        "colab_type": "text"
      },
      "source": [
        "### XOR\n",
        "Use TwoLayerNet to train the XOR function discussed in the class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeUsLKYEwzTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "9c306ba5-9f1c-44aa-f9b8-51ebd39d69cd"
      },
      "source": [
        "### Write your code here\n",
        "\n",
        "X = np.array([\n",
        "              [0,0,0],\n",
        "              [0,0,1],\n",
        "              [0,1,0],\n",
        "              [0,1,1],\n",
        "              [1,0,0],\n",
        "              [1,0,1],\n",
        "              [1,1,0],\n",
        "              [1,1,1]\n",
        "])\n",
        "\n",
        "y = np.array([0,1,1,0,1,0,0,1])\n",
        "\n",
        "np.random.shuffle(X)\n",
        "state = np.random.get_state()\n",
        "np.random.set_state(state)\n",
        "np.random.shuffle(y)\n",
        "\n",
        "X_train, X_val = X[:6], X[6:]\n",
        "y_train, y_val = y[:6], y[6:]\n",
        "\n",
        "xor_net = TwoLayerNet(3, 5, 2)\n",
        "\n",
        "results = xor_net.train(X_train, y_train, X_val, y_val,\n",
        "              learning_rate=0.01, num_iters=10000,\n",
        "              batch_size=4, verbose=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(results['loss_history'])\n",
        "print(results['loss_history'][-10:])\n",
        "xor_net.predict(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.09931287050361885, 0.09931934106132823, 0.09932581182589902, 0.09933228279718062, 0.09933875397502256, 0.09934522535927462, 0.09935169694978627, 0.09935816874640727, 0.09936464074898765, 0.0993711129573771]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 1, 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3D4GjcImFbW",
        "colab_type": "text"
      },
      "source": [
        "### Iris\n",
        "Use TwoLayerNet to train the iris dataset. Choose 120 samples randomly for training and the rest for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSjLcJT1mX-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "f77f30ae-bcda-4027-faa5-e1aad10bb26a"
      },
      "source": [
        "### Write your code here\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "data, target = load_iris(return_X_y=True)\n",
        "\n",
        "np.random.shuffle(data)\n",
        "state = np.random.get_state()\n",
        "np.random.set_state(state)\n",
        "np.random.shuffle(target)\n",
        "\n",
        "data_train = data[:120]\n",
        "target_train = target[:120]\n",
        "\n",
        "data_test = data[120:]\n",
        "target_test = target[120:]\n",
        "\n",
        "iris_net = TwoLayerNet(4, 64, 3)\n",
        "results = iris_net.train(data_train, target_train, data_test, target_test,\n",
        "              learning_rate=0.05, num_iters=4000,\n",
        "              batch_size=60, verbose=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(results['loss_history'])\n",
        "print(results['loss_history'][-10:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0260511822368723, 1.026051116446566, 1.0260510767093336, 1.0260510436969077, 1.0260510140767156, 1.0260509874930548, 1.026050964414644, 1.0260509444462476, 1.0260509279535979, 1.0260509145604926]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbFD9EXlmaDI",
        "colab_type": "text"
      },
      "source": [
        "### Advanced\n",
        "Add weight regularization to the loss and rewrite backprop part of TwoLayerNet. <br>\n",
        "Train using some datasets and see if regularized network performs better than its older counterpart.\n",
        "<br>\n",
        "The expression for loss with regularization is as follows - <br>\n",
        "$L = -\\sum{t_i \\log{p_i}} + \\lambda(|w_1|^2 + |w_2|^2)$ <br>\n",
        "$\\lambda$ is a tunable hyper-parameter  denoting strength of regularization. <br>\n",
        "If it is too high, network will struggle to fit, and if it is too low, network will overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBAJhyB9oB8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Write your code here\n",
        "class TwoLayerNet(TwoLayerNet):    \n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute the loss and gradients for a two layer fully connected neural\n",
        "        network.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "          an integer in the range 0 <= y[i] < C.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
        "        the score for class c on input X[i].\n",
        "\n",
        "        If y is not None, instead return a tuple of:\n",
        "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "          samples. (This is the mean loss over N samples)\n",
        "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "          with respect to the loss function; has the same keys as self.params.\n",
        "        \"\"\"\n",
        "        # Unpack variables from the params dictionary\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        N, D = X.shape\n",
        "        H, C = W2.shape\n",
        "        \n",
        "        # Compute the forward pass\n",
        "        scores = None\n",
        "        #############################################################################\n",
        "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
        "        # Store the result in the scores variable, which should be an array of      #\n",
        "        # shape (N, C).                                                             #\n",
        "        #############################################################################\n",
        "        \n",
        "        \n",
        "        \n",
        "        ## Write your code here\n",
        "        \n",
        "        v1 = np.dot(X, W1) + b1\n",
        "        h1 = np.maximum(v1, 0)\n",
        "        v2 = np.dot(h1, W2) + b2\n",
        "        h2 = []\n",
        "        for i in range(v2.shape[0]):\n",
        "          h2.append(np.exp(v2[i])/sum(np.exp(v2[i])))\n",
        "        scores = np.asarray(h2)\n",
        "\n",
        "        if(y.all()==None):\n",
        "          return scores\n",
        "        \n",
        "        # # Compute the loss\n",
        "        loss = None\n",
        "\n",
        "        \n",
        "        #############################################################################\n",
        "        # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
        "        # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
        "        # in the variable loss, which should be a scalar. Use the Categorical       #\n",
        "        # Cross Entropy loss.                                                       #\n",
        "        #############################################################################\n",
        "      \n",
        "        ### Write your code here\n",
        "\n",
        "        t_i = np.zeros((N,C), dtype=np.int_)\n",
        "        for i in range(N):\n",
        "          for j in range(C):\n",
        "            if (j==y[i]):\n",
        "              t_i[i,j] = 1\n",
        "\n",
        "        data_loss = np.sum(np.sum(t_i*np.log(scores)))\n",
        "\n",
        "        data_loss = -data_loss/N\n",
        "        lam = 0.001\n",
        "\n",
        "        l2 = (lam * np.sum(W1** 2)) + (lam * np.sum(W2 ** 2))\n",
        "\n",
        "        loss = data_loss + l2\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        grads = {}\n",
        "        \n",
        "        #############################################################################\n",
        "        # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
        "        # and biases. Store the results in the grads dictionary. For example,       #\n",
        "        # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
        "        #############################################################################\n",
        "        \n",
        "        ### Write your code here\n",
        "\n",
        "        output_error = (scores-t_i)/scores.shape[0] #dl/d(softmax) = (pi - yi)/N\n",
        "        grad_W2 = np.dot(h1.T, output_error)\n",
        "        grad_b2 = np.sum(output_error, axis=0)\n",
        "        hidden_error = np.dot((scores-t_i)/scores.shape[0], W2.T)\n",
        "        grad_W1 = np.dot(X.T, hidden_error)\n",
        "        grad_b1 = np.sum(np.dot(X.T, hidden_error), axis=0)\n",
        "\n",
        "        grads = {\n",
        "            \"W1\": grad_W1,\n",
        "            \"b1\": grad_b1,\n",
        "            \"W2\": grad_W2,\n",
        "            \"b2\": grad_b2\n",
        "        }\n",
        "\n",
        "        return loss, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZBgaO1dwQGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}